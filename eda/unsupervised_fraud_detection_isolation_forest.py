{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n##All General Import Statements\nimport pandas as pd\nimport numpy as np\nimport math\nimport random\n%matplotlib inline\nimport random\nfrom matplotlib import pyplot\nimport os\nprint(os.listdir(\"../input\"))\n\n# %% [markdown]\n# # Anomaly Detection Algorithms: Isolation Forest vs the Rest\n\n# %% [markdown]\n# <p>This notebook shows a simplified implementation of the algorithm Isolation Forest and compares its Scikit-learn implementation with other popular anomaly detection algorithms. (KMeans, Local Outlier Factor, One-Class SVM)</p>\n# \n# <p>In a real world scenario, an unsupervised model is used primarily as a seed to create labelled data unless risk rules based on domain knowledge can be formulated for the problem. For example, if the problem is to identify anomalies in network traffic metrics such as time between logins and distance between origins can be used to formulate a risk rule. Anomalous logins obtained by applying this rule must then be audited to come up with their true labels. On the other hand take the case of identifying the probability of an employee  committing securities fraud. Here the behavioral data that the organization captures is very high dimensional and the relationship between the data points is complex. Hence without indepth domain knowledge, formulating risk rules is difficult.  This combined with issues such as confidentiality makes it very hard to validate these risk rules.</p>\n# \n# <p>This is where unsupervised learning shines. With very little domain knowledge, a simple unsupervised algorithm can be used to create a list of anomalies which can then be fed into an auditing process which then generates the true labels. Over time when enough data labels are created, the unsupervised problem can be reformulated as a supervised ML problem.</p>\n# \n# <p>Since this notebook aims to compare various unsupervised algorithms with each other their actual performance needs to be validated. For this I use a labelled dataset. But the algorithms do not see the labels while training. The labels are only used to compare the model predictions to the actual values and to create performance metrics.</p>\n\n# %% [markdown]\n# ## Isolation Forests in Python\n\n# %% [markdown]\n# The Algorithm has 3 parts:\n#     1. Forest\n#     2. Isolation Tree\n#     3. Evaluation (Path Length)\n\n# %% [markdown]\n# ### Common Classes\n\n# %% [markdown]\n# The below code defines classes for external and internal nodes\n\n# %% [code]\nclass ExNode:\n    def __init__(self,size):\n        self.size=size\n        \nclass InNode:\n    def __init__(self,left,right,splitAtt,splitVal):\n        self.left=left\n        self.right=right\n        self.splitAtt=splitAtt\n        self.splitVal=splitVal\n\n# %% [markdown]\n# ### Forest\n\n# %% [code]\ndef iForest(X,noOfTrees,sampleSize):\n    forest=[]\n    hlim=math.ceil(math.log(sampleSize,2))\n    for i in range(noOfTrees):\n        X_train=X.sample(sampleSize)\n        forest.append(iTree(X_train,0,hlim))\n    return forest\n\n# %% [markdown]\n# ### Isolation Tree\n\n# %% [code]\ndef iTree(X,currHeight,hlim):\n    if currHeight>=hlim or len(X)<=1:\n        return ExNode(len(X))\n    else:\n        Q=X.columns\n        q=random.choice(Q)\n        p=random.choice(X[q].unique())\n        X_l=X[X[q]<p]\n        X_r=X[X[q]>=p]\n        return InNode(iTree(X_l,currHeight+1,hlim),iTree(X_r,currHeight+1,hlim),q,p)\n\n# %% [markdown]\n# ### Path Length\n\n# %% [code]\ndef pathLength(x,Tree,currHeight):\n    if isinstance(Tree,ExNode):\n        return currHeight\n    a=Tree.splitAtt\n    if x[a]<Tree.splitVal:\n        return pathLength(x,Tree.left,currHeight+1)\n    else:\n        return pathLength(x,Tree.right,currHeight+1)\n\n# %% [markdown]\n# ## Test Run\n\n# %% [markdown]\n# Let us now test the algorithm on a test dataset.\n# Source: https://www.kaggle.com/dalpozz/creditcardfraud\n\n# %% [code]\ndf=pd.read_csv(\"../input/creditcard.csv\")\ny_true=df['Class']\ndf_data=df.drop('Class',1)\n\n# %% [markdown]\n# Next, we create the forest.\n\n# %% [code]\nsampleSize=10000\nifor=iForest(df_data.sample(100000),10,sampleSize) ##Forest of 10 trees\n\n# %% [markdown]\n# Next, we select 1000 random datapoints and get their path lengths. The purpose for this is to plot and see if anomalies actually have shorter path lengths.\n\n# %% [code]\nposLenLst=[]\nnegLenLst=[]\n\nfor sim in range(1000):\n    ind=random.choice(df_data[y_true==1].index)\n    for tree in ifor:\n        posLenLst.append(pathLength(df_data.iloc[ind],tree,0))\n        \n    ind=random.choice(df_data[y_true==0].index)\n    for tree in ifor:\n        negLenLst.append(pathLength(df_data.iloc[ind],tree,0))\n\n# %% [markdown]\n# Finally, we plot the path lengths.\n\n# %% [code]\nbins = np.linspace(0,math.ceil(math.log(sampleSize,2)), math.ceil(math.log(sampleSize,2)))\n\npyplot.figure(figsize=(12,8))\npyplot.hist(posLenLst, bins, alpha=0.5, label='Anomaly')\npyplot.hist(negLenLst, bins, alpha=0.5, label='Normal')\npyplot.xlabel('Path Length')\npyplot.ylabel('Frequency')\npyplot.legend(loc='upper left')\n\n# %% [markdown]\n# Anomalies do seem to have a lower path length. Not bad for random division!\n\n# %% [markdown]\n# #### Notes:\n\n# %% [markdown]\n# The above implementation ignores three aspects of the actual algorithm fo the sake of simplicity.\n# 1. The average depth needs to be added to the depth once the current length hits the height limit\n# 2. The path lengths are not normalized between trees and hence the actual values are used for plotting\n# 3. The authors of the paper suggest using kurtosis to select features as a refinement over random selection\n\n# %% [markdown]\n# ## Plotting the Data\n\n# %% [markdown]\n# Using a technique called T-SNE, we can reduce the dimensions of the data and create a 2D plot. The objective here is to show that distance based anomaly detection methods might not work as well as other techniques on this dataset. This is because the positive cases are not too far away from the normal cases.\n\n# %% [code]\nfrom sklearn.manifold import TSNE\n\n# %% [code]\ndf_plt=df[df['Class']==0].sample(1000)\ndf_plt_pos=df[df['Class']==1].sample(20)\ndf_plt=pd.concat([df_plt,df_plt_pos])\ny_plt=df_plt['Class']\nX_plt=df_plt.drop('Class',1)\n\n# %% [code]\nX_embedded = TSNE(n_components=2).fit_transform(X_plt)\n\n# %% [code]\npyplot.figure(figsize=(12,8))\npyplot.scatter(X_embedded[:,0], X_embedded[:,1], c=y_plt, cmap=pyplot.cm.get_cmap(\"Paired\", 2))\npyplot.colorbar(ticks=range(2))\n\n# %% [markdown]\n# ## Time for the Real Fight!\n\n# %% [markdown]\n# To keep things even, all of the algorithms are run with their default parameters.\n\n# %% [markdown]\n# Let's start by importing the scikit-learn implementations of all 4 algorithms.\n\n# %% [code]\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.neighbors import LocalOutlierFactor ## Only available with scikit-learn 0.19 and later\nfrom sklearn.cluster import KMeans\n\n# %% [markdown]\n# Next, let's create a train and test dataset.\n\n# %% [code]\nfrom sklearn.model_selection import train_test_split\n\n# %% [code]\nX_train, X_test, y_train, y_test = train_test_split(df_data, y_true, test_size=0.3, random_state=42)\n\n# %% [markdown]\n# Finally, let's create a few helper functions that help with training and testing the models. The preprocess function is not used in this notebook but it might help improve the scores on the KMeans and One Class SVM models.\n\n# %% [code]\n## Not required for Isolation Forest\ndef preprocess(df_data):\n    for col in df_data:\n        df_data[col]=(df_data[col]-np.min(df_data[col]))/(np.max(df_data[col])-np.min(df_data[col]))\n    return\n\n# %% [markdown]\n# <b>Note:</b> The below train and predict functions are designed to output ensemble models (bagged models), with the default size being 5 models. The Isolation Forest and One Class SVM use these functions.\n\n# %% [code]\n## Not valid for LOF\ndef train(X,clf,ensembleSize=5,sampleSize=10000):\n    mdlLst=[]\n    for n in range(ensembleSize):\n        X=df_data.sample(sampleSize)\n        clf.fit(X)\n        mdlLst.append(clf)\n    return mdlLst\n\n# %% [code]\n## Not valif for LOF\ndef predict(X,mdlLst):\n    y_pred=np.zeros(X.shape[0])\n    for clf in mdlLst:\n        y_pred=np.add(y_pred,clf.decision_function(X).reshape(X.shape[0],))\n    y_pred=(y_pred*1.0)/len(mdlLst)\n    return y_pred\n\n# %% [markdown]\n# Finally, let's import some model scoring libraries. Since, we are dealing with a heavily imbalanced dataset, F1 Score is used as a proxy for model performance.\n\n# %% [markdown]\n# For more details, refer http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n\n# %% [code]\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix,f1_score\n\n# %% [markdown]\n# #### Isolation Forest\n\n# %% [code]\nalg=IsolationForest(n_estimators=100, max_samples='auto', contamination=0.01, \\\n                        max_features=1.0, bootstrap=False, n_jobs=-1, random_state=42, verbose=0,behaviour=\"new\")\n\n# %% [markdown]\n# <b>Note:</b> The magic function timeit does not let us use any variable that is created in the timeit cell. Hence, every cell with a timeit magic function will have a corresponding regular cell with the same code.\n\n# %% [code]\n%%timeit\nif_mdlLst=train(X_train,alg)\n\n# %% [code]\nif_mdlLst=train(X_train,alg)\n\n# %% [code]\n%%timeit\nif_y_pred=predict(X_test,if_mdlLst)\nif_y_pred=1-if_y_pred\n\n#Creating class labels based on decision function\nif_y_pred_class=if_y_pred.copy()\nif_y_pred_class[if_y_pred>=np.percentile(if_y_pred,95)]=1\nif_y_pred_class[if_y_pred<np.percentile(if_y_pred,95)]=0\n\n# %% [code]\nif_y_pred=predict(X_test,if_mdlLst)\nif_y_pred=1-if_y_pred\n\n#Creating class labels based on decision function\nif_y_pred_class=if_y_pred.copy()\nif_y_pred_class[if_y_pred>=np.percentile(if_y_pred,95)]=1\nif_y_pred_class[if_y_pred<np.percentile(if_y_pred,95)]=0\n\n# %% [code]\nroc_auc_score(y_test, if_y_pred_class)\n\n# %% [code]\nf1_score(y_test, if_y_pred_class)\n\n# %% [code]\nif_cm=confusion_matrix(y_test, if_y_pred_class)\n\n# %% [code]\nimport seaborn as sn\n     \ndf_cm = pd.DataFrame(if_cm,\n                  ['True Normal','True Fraud'],['Pred Normal','Pred Fraud'])\npyplot.figure(figsize = (8,4))\nsn.set(font_scale=1.4)#for label size\nsn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16},fmt='g')# font size\n\n# %% [markdown]\n# #### KMeans\n\n# %% [markdown]\n# The top 5% of the farthest point from cluster centers are itentified as fraud (outliers)\n\n# %% [code]\n%%timeit\n\nkmeans = KMeans(n_clusters=8, random_state=42,n_jobs=-1).fit(X_train)\n\n# %% [code]\nkmeans = KMeans(n_clusters=8, random_state=42,n_jobs=-1).fit(X_train)\n\n# %% [code]\n%%timeit\nX_test_clusters=kmeans.predict(X_test)\nX_test_clusters_centers=kmeans.cluster_centers_\ndist = [np.linalg.norm(x-y) for x,y in zip(X_test.values,X_test_clusters_centers[X_test_clusters])]\n\nkm_y_pred=np.array(dist)\nkm_y_pred[dist>=np.percentile(dist,95)]=1\nkm_y_pred[dist<np.percentile(dist,95)]=0\n\n# %% [code]\nX_test_clusters=kmeans.predict(X_test)\nX_test_clusters_centers=kmeans.cluster_centers_\ndist = [np.linalg.norm(x-y) for x,y in zip(X_test.values,X_test_clusters_centers[X_test_clusters])]\n\nkm_y_pred=np.array(dist)\nkm_y_pred[dist>=np.percentile(dist,95)]=1\nkm_y_pred[dist<np.percentile(dist,95)]=0\n\n# %% [code]\nroc_auc_score(y_test, km_y_pred)\n\n# %% [code]\nf1_score(y_test, km_y_pred)\n\n# %% [code]\nkm_cm=confusion_matrix(y_test, km_y_pred)\n\n# %% [code]\ndf_cm = pd.DataFrame(km_cm,\n                  ['True Normal','True Fraud'],['Pred Normal','Pred Fraud'])\npyplot.figure(figsize = (8,4))\nsn.set(font_scale=1.4)#for label size\nsn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16},fmt='g')# font size\n\n# %% [markdown]\n# #### LOF\n\n# %% [markdown]\n# Local Outlier Factor only looks at the local neighbourhood of a data point and hence cannot make predictions on out of sample data points. Hence we work directly with X_test here.\n\n# %% [code]\nclf=LocalOutlierFactor(n_neighbors=20, algorithm='auto', leaf_size=30, \\\n                   metric='minkowski', p=2, metric_params=None, contamination=0.1, n_jobs=-1)\n\n# %% [code]\n%%timeit\nclf.fit(X_test)\n\n# %% [code]\nclf.fit(X_test)\n\n# %% [code]\n%%timeit\nlof_y_pred=clf.negative_outlier_factor_\n\n#Creating class labels based on decision function\nlof_y_pred_class=lof_y_pred.copy()\nlof_y_pred_class[lof_y_pred>=np.percentile(lof_y_pred,95)]=1\nlof_y_pred_class[lof_y_pred<np.percentile(lof_y_pred,95)]=0\n\n# %% [code]\nlof_y_pred=clf.negative_outlier_factor_\n\n#Creating class labels based on decision function\nlof_y_pred_class=lof_y_pred.copy()\nlof_y_pred_class[lof_y_pred>=np.percentile(lof_y_pred,95)]=1\nlof_y_pred_class[lof_y_pred<np.percentile(lof_y_pred,95)]=0\n\n# %% [code]\nroc_auc_score(y_test, lof_y_pred_class)\n\n# %% [code]\nf1_score(y_test, lof_y_pred_class)\n\n# %% [code]\nlof_cm=confusion_matrix(y_test, lof_y_pred_class)\n\n# %% [code]\ndf_cm = pd.DataFrame(lof_cm,\n                  ['True Normal','True Fraud'],['Pred Normal','Pred Fraud'])\npyplot.figure(figsize = (8,4))\nsn.set(font_scale=1.4)#for label size\nsn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16},fmt='g')# font size\n\n# %% [markdown]\n# #### One-Class SVM*\n\n# %% [markdown]\n# ##### *Any kernel except linear will take a while. Feel free to grab a coffee, if you decide to change the kernel type\n\n# %% [code]\nalg=OneClassSVM(kernel='linear',gamma='auto', coef0=0.0, tol=0.001, nu=0.5, \\\n                shrinking=True, cache_size=500, verbose=False, max_iter=-1)\n\n# %% [code]\n%%timeit\nosvm_mdlLst=train(X_train,alg)\n\n# %% [code]\nosvm_mdlLst=train(X_train,alg)\n\n# %% [code]\n%%timeit\nosvm_y_pred=predict(X_test,osvm_mdlLst)\n\n#Creating class labels based on decision function\nosvm_y_pred_class=osvm_y_pred.copy()\nosvm_y_pred_class[osvm_y_pred<0]=1\nosvm_y_pred_class[osvm_y_pred>=0]=0\n\n# %% [code]\nosvm_y_pred=predict(X_test,osvm_mdlLst)\n\n#Creating class labels based on decision function\nosvm_y_pred_class=osvm_y_pred.copy()\nosvm_y_pred_class[osvm_y_pred<0]=1\nosvm_y_pred_class[osvm_y_pred>=0]=0\n\n# %% [code]\nroc_auc_score(y_test, osvm_y_pred_class)\n\n# %% [code]\nf1_score(y_test, osvm_y_pred_class)\n\n# %% [code]\nosvm_cm=confusion_matrix(y_test, osvm_y_pred_class)\n\n# %% [code]\ndf_cm = pd.DataFrame(osvm_cm,\n                  ['True Normal','True Fraud'],['Pred Normal','Pred Fraud'])\npyplot.figure(figsize = (8,4))\nsn.set(font_scale=1.4)#for label size\nsn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16},fmt='g')# font size\n\n# %% [markdown]\n# ## Conclusion\n\n# %% [code]\n## Performance Plot\ntrain_times={\n    'Isolation Forest': 4.38,\n    'KMeans':20.9,\n    'LOF':2.06,\n    'OneClass SVM': 21.4\n}\n\nprediction_times={\n    'Isolation Forest':34.2,\n    'KMeans':0.052,\n    'LOF':0.00368,\n    'OneClass SVM': 71\n}\n\n# %% [code]\npyplot.title('Training Time')\npyplot.barh(range(len(train_times)), list(train_times.values()), align='center')\npyplot.yticks(range(len(train_times)), list(train_times.keys()))\npyplot.xlabel('Time in seconds')\n\n# %% [markdown]\n# Isolation forest has the 2nd fastest training time. This is very impressive considering that our model is actually an ensemble of 5 Isolation forest models.\n\n# %% [code]\npyplot.title('Prediction Time')\npyplot.barh(range(len(prediction_times)), list(prediction_times.values()), align='center')\npyplot.yticks(range(len(prediction_times)), list(prediction_times.keys()))\npyplot.xlabel('Time in seconds')\n\n# %% [markdown]\n# The algorithm doesn't fare so well but this might be due to the over head of the ensemble structure.\n\n# %% [code]\n## Performance Plot\nauc_scores={\n    'Isolation Forest': roc_auc_score(y_test, if_y_pred_class),\n    'KMeans':roc_auc_score(y_test, km_y_pred),\n    'LOF':roc_auc_score(y_test, lof_y_pred_class),\n    'OneClass SVM': roc_auc_score(y_test, osvm_y_pred_class)\n}\n\nf1_scores={\n    'Isolation Forest':f1_score(y_test, if_y_pred_class),\n    'KMeans':f1_score(y_test, km_y_pred),\n    'LOF':f1_score(y_test, lof_y_pred_class),\n    'OneClass SVM': f1_score(y_test, osvm_y_pred_class)\n}\n\n# %% [code]\npyplot.title('AUC Scores')\npyplot.barh(range(len(auc_scores)), list(auc_scores.values()), align='center')\npyplot.yticks(range(len(auc_scores)), list(auc_scores.keys()))\npyplot.xlabel('AUC Score')\n\n# %% [code]\npyplot.title('F1 Scores')\npyplot.barh(range(len(f1_scores)), list(f1_scores.values()), align='center')\npyplot.yticks(range(len(f1_scores)), list(f1_scores.keys()))\npyplot.xlabel('F1 Score')\n\n# %% [markdown]\n# Coming to model performance, our isolation forest ensemble model is a clear winner!\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":true}}\n","metadata":{"_uuid":"870e5d67-da22-4326-880e-187c98e527b2","_cell_guid":"535ac6c2-1182-4795-bf36-66a1d67721bc","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}